{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borch_and/Documents/GitHub/SecondYearProject/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from TokenClassificationTrainer import TokenClassificationTrainer\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.49s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2687988877296448, 'eval_precision': 0.6567164179104478, 'eval_recall': 0.5751633986928104, 'eval_f1': 0.613240418118467, 'eval_span_f1': 0.6454652532391049, 'eval_accuracy': 0.9194109224790346, 'eval_runtime': 15.2357, 'eval_samples_per_second': 18.706, 'eval_steps_per_second': 0.591}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [06:49<00:00,  2.56s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3745052218437195, 'eval_precision': 0.5371530886302597, 'eval_recall': 0.4622941346431667, 'eval_f1': 0.49692013044153427, 'eval_span_f1': 0.5365136072291689, 'eval_accuracy': 0.9182330531154561, 'eval_runtime': 410.7069, 'eval_samples_per_second': 12.418, 'eval_steps_per_second': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:43<00:00,  2.39s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3405876159667969, 'eval_precision': 0.31990794016110474, 'eval_recall': 0.2883817427385892, 'eval_f1': 0.3033278777959629, 'eval_span_f1': 0.32066115702479336, 'eval_accuracy': 0.9191772041256338, 'eval_runtime': 45.3779, 'eval_samples_per_second': 12.473, 'eval_steps_per_second': 0.397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:25<00:00,  5.02s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.696006178855896, 'eval_precision': 0.08434081750143926, 'eval_recall': 0.10965568862275449, 'eval_f1': 0.09534656687276277, 'eval_span_f1': 0.11408614668218858, 'eval_accuracy': 0.8670182570638539, 'eval_runtime': 149.1533, 'eval_samples_per_second': 6.208, 'eval_steps_per_second': 0.194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the task and name of the pretrained model and the batch size for finetuning\n",
    "task = \"ner\"\n",
    "model_name = \"xlm-mlm-17-1280\"\n",
    "save_name = \"baseline\"\n",
    "batch_size = 32\n",
    "\n",
    "# Flag to indicate whether to label all tokens or just the first token of each word\n",
    "label_all_tokens = True\n",
    "\n",
    "# File paths to splits of the chosen dataset\n",
    "file_paths = {\n",
    "    \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "    \"validation\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "}\n",
    "\n",
    "trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "# load trianed model to trainer\n",
    "trainer.set_trainer(use_old = True)\n",
    "evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "baseline_eval_baseline_model = evals[0]\n",
    "NoStaD_eval_baseline_model = evals[1]\n",
    "DaNplus_eval_baseline_model = evals[2]\n",
    "Hungarian_eval_baseline_model = evals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Language</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>English</td>\n",
       "      <td>0.268799</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.575163</td>\n",
       "      <td>0.613240</td>\n",
       "      <td>0.645465</td>\n",
       "      <td>0.919411</td>\n",
       "      <td>15.2357</td>\n",
       "      <td>18.706</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSta-D</td>\n",
       "      <td>German</td>\n",
       "      <td>0.374505</td>\n",
       "      <td>0.537153</td>\n",
       "      <td>0.462294</td>\n",
       "      <td>0.496920</td>\n",
       "      <td>0.536514</td>\n",
       "      <td>0.918233</td>\n",
       "      <td>410.7069</td>\n",
       "      <td>12.418</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaNplus</td>\n",
       "      <td>Danish</td>\n",
       "      <td>0.340588</td>\n",
       "      <td>0.319908</td>\n",
       "      <td>0.288382</td>\n",
       "      <td>0.303328</td>\n",
       "      <td>0.320661</td>\n",
       "      <td>0.919177</td>\n",
       "      <td>45.3779</td>\n",
       "      <td>12.473</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>Hungarian</td>\n",
       "      <td>0.696006</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.109656</td>\n",
       "      <td>0.095347</td>\n",
       "      <td>0.114086</td>\n",
       "      <td>0.867018</td>\n",
       "      <td>149.1533</td>\n",
       "      <td>6.208</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset   Language  eval_loss  eval_precision  eval_recall   eval_f1  \\\n",
       "0   Baseline    English   0.268799        0.656716     0.575163  0.613240   \n",
       "1    NoSta-D     German   0.374505        0.537153     0.462294  0.496920   \n",
       "2    DaNplus     Danish   0.340588        0.319908     0.288382  0.303328   \n",
       "3  Hungarian  Hungarian   0.696006        0.084341     0.109656  0.095347   \n",
       "\n",
       "   eval_span_f1  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0      0.645465       0.919411       15.2357                   18.706   \n",
       "1      0.536514       0.918233      410.7069                   12.418   \n",
       "2      0.320661       0.919177       45.3779                   12.473   \n",
       "3      0.114086       0.867018      149.1533                    6.208   \n",
       "\n",
       "   eval_steps_per_second  \n",
       "0                  0.591  \n",
       "1                  0.390  \n",
       "2                  0.397  \n",
       "3                  0.194  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Dataset\", \"Language\"] + [name for name, _ in baseline_eval_baseline_model.items()]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Add the evals to df\n",
    "df.loc[0] = [\"Baseline\", \"English\"] + [value for _, value in baseline_eval_baseline_model.items()]\n",
    "df.loc[1] = [\"NoSta-D\", \"German\"] + [value for _, value in NoStaD_eval_baseline_model.items()]\n",
    "df.loc[2] = [\"DaNplus\", \"Danish\"] + [value for _, value in DaNplus_eval_baseline_model.items()]\n",
    "df.loc[3] = [\"Hungarian\", \"Hungarian\"] + [value for _, value in Hungarian_eval_baseline_model.items()]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.47s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26985597610473633, 'eval_precision': 0.6788732394366197, 'eval_recall': 0.5250544662309368, 'eval_f1': 0.5921375921375921, 'eval_span_f1': 0.6268656716417911, 'eval_accuracy': 0.9175700552260176, 'eval_runtime': 15.4004, 'eval_samples_per_second': 18.506, 'eval_steps_per_second': 0.584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [04:39<00:00,  1.75s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31536611914634705, 'eval_precision': 0.5878039367039753, 'eval_recall': 0.44004622941346433, 'eval_f1': 0.5033046926635824, 'eval_span_f1': 0.5517472367980352, 'eval_accuracy': 0.9219404300872138, 'eval_runtime': 281.4085, 'eval_samples_per_second': 18.123, 'eval_steps_per_second': 0.569}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:43<00:00,  2.40s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27991461753845215, 'eval_precision': 0.44309559939301973, 'eval_recall': 0.3029045643153527, 'eval_f1': 0.35982747997535425, 'eval_span_f1': 0.39274546591619763, 'eval_accuracy': 0.9294912883864577, 'eval_runtime': 45.6087, 'eval_samples_per_second': 12.41, 'eval_steps_per_second': 0.395}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:26<00:00,  5.07s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49252477288246155, 'eval_precision': 0.11394101876675604, 'eval_recall': 0.09543413173652694, 'eval_f1': 0.10386965376782077, 'eval_span_f1': 0.12744689712619742, 'eval_accuracy': 0.8861726089106191, 'eval_runtime': 150.6294, 'eval_samples_per_second': 6.148, 'eval_steps_per_second': 0.193}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the task and name of the pretrained model and the batch size for finetuning\n",
    "task = \"ner\"\n",
    "model_name = \"xlm-mlm-17-1280\"\n",
    "save_name = \"scheduler\"\n",
    "batch_size = 32\n",
    "\n",
    "# Flag to indicate whether to label all tokens or just the first token of each word\n",
    "label_all_tokens = True\n",
    "\n",
    "# File paths to splits of the chosen dataset\n",
    "file_paths = {\n",
    "    \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "    \"validation\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "}\n",
    "\n",
    "trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "# load trianed model to trainer\n",
    "trainer.set_trainer(use_old = True)\n",
    "evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "baseline_eval_scheduler_model = evals[0]\n",
    "NoStaD_eval_scheduler_model = evals[1]\n",
    "DaNplus_eval_scheduler_model = evals[2]\n",
    "Hungarian_eval_scheduler_model = evals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Language</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>English</td>\n",
       "      <td>0.269856</td>\n",
       "      <td>0.678873</td>\n",
       "      <td>0.525054</td>\n",
       "      <td>0.592138</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.917570</td>\n",
       "      <td>15.4004</td>\n",
       "      <td>18.506</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSta-D</td>\n",
       "      <td>German</td>\n",
       "      <td>0.315366</td>\n",
       "      <td>0.587804</td>\n",
       "      <td>0.440046</td>\n",
       "      <td>0.503305</td>\n",
       "      <td>0.551747</td>\n",
       "      <td>0.921940</td>\n",
       "      <td>281.4085</td>\n",
       "      <td>18.123</td>\n",
       "      <td>0.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaNplus</td>\n",
       "      <td>Danish</td>\n",
       "      <td>0.279915</td>\n",
       "      <td>0.443096</td>\n",
       "      <td>0.302905</td>\n",
       "      <td>0.359827</td>\n",
       "      <td>0.392745</td>\n",
       "      <td>0.929491</td>\n",
       "      <td>45.6087</td>\n",
       "      <td>12.410</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>Hungarian</td>\n",
       "      <td>0.492525</td>\n",
       "      <td>0.113941</td>\n",
       "      <td>0.095434</td>\n",
       "      <td>0.103870</td>\n",
       "      <td>0.127447</td>\n",
       "      <td>0.886173</td>\n",
       "      <td>150.6294</td>\n",
       "      <td>6.148</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset   Language  eval_loss  eval_precision  eval_recall   eval_f1  \\\n",
       "0   Baseline    English   0.269856        0.678873     0.525054  0.592138   \n",
       "1    NoSta-D     German   0.315366        0.587804     0.440046  0.503305   \n",
       "2    DaNplus     Danish   0.279915        0.443096     0.302905  0.359827   \n",
       "3  Hungarian  Hungarian   0.492525        0.113941     0.095434  0.103870   \n",
       "\n",
       "   eval_span_f1  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0      0.626866       0.917570       15.4004                   18.506   \n",
       "1      0.551747       0.921940      281.4085                   18.123   \n",
       "2      0.392745       0.929491       45.6087                   12.410   \n",
       "3      0.127447       0.886173      150.6294                    6.148   \n",
       "\n",
       "   eval_steps_per_second  \n",
       "0                  0.584  \n",
       "1                  0.569  \n",
       "2                  0.395  \n",
       "3                  0.193  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Dataset\", \"Language\"] + [name for name, _ in baseline_eval_scheduler_model.items()]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Add the evals to df\n",
    "df.loc[0] = [\"Baseline\", \"English\"] + [value for _, value in baseline_eval_scheduler_model.items()]\n",
    "df.loc[1] = [\"NoSta-D\", \"German\"] + [value for _, value in NoStaD_eval_scheduler_model.items()]\n",
    "df.loc[2] = [\"DaNplus\", \"Danish\"] + [value for _, value in DaNplus_eval_scheduler_model.items()]\n",
    "df.loc[3] = [\"Hungarian\", \"Hungarian\"] + [value for _, value in Hungarian_eval_scheduler_model.items()]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval discriminate_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.47s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28819602727890015, 'eval_precision': 0.6292906178489702, 'eval_recall': 0.599128540305011, 'eval_f1': 0.6138392857142857, 'eval_span_f1': 0.6507936507936508, 'eval_accuracy': 0.9177745960319084, 'eval_runtime': 15.3079, 'eval_samples_per_second': 18.618, 'eval_steps_per_second': 0.588}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [04:52<00:00,  1.83s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36379221081733704, 'eval_precision': 0.5688515406162465, 'eval_recall': 0.48897235866319944, 'eval_f1': 0.5258960016573442, 'eval_span_f1': 0.5638859556494192, 'eval_accuracy': 0.9220112716854003, 'eval_runtime': 293.5972, 'eval_samples_per_second': 17.371, 'eval_steps_per_second': 0.545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:44<00:00,  2.49s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3233060836791992, 'eval_precision': 0.3349950149551346, 'eval_recall': 0.34854771784232363, 'eval_f1': 0.3416370106761566, 'eval_span_f1': 0.3737738771295818, 'eval_accuracy': 0.9230814055124993, 'eval_runtime': 47.3114, 'eval_samples_per_second': 11.963, 'eval_steps_per_second': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:29<00:00,  5.17s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.65663081407547, 'eval_precision': 0.08034006376195536, 'eval_recall': 0.14146706586826346, 'eval_f1': 0.1024806832045547, 'eval_span_f1': 0.11441963054866282, 'eval_accuracy': 0.8549140251606878, 'eval_runtime': 153.7344, 'eval_samples_per_second': 6.023, 'eval_steps_per_second': 0.189}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the task and name of the pretrained model and the batch size for finetuning\n",
    "task = \"ner\"\n",
    "model_name = \"xlm-mlm-17-1280\"\n",
    "save_name = \"discriminate_lr\"\n",
    "batch_size = 32\n",
    "\n",
    "# Flag to indicate whether to label all tokens or just the first token of each word\n",
    "label_all_tokens = True\n",
    "\n",
    "# File paths to splits of the chosen dataset\n",
    "file_paths = {\n",
    "    \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "    \"validation\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "}\n",
    "\n",
    "trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "# load trianed model to trainer\n",
    "trainer.set_trainer(use_old = True)\n",
    "evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "baseline_eval_discriminate_lr_model = evals[0]\n",
    "NoStaD_eval_discriminate_lr_model = evals[1]\n",
    "DaNplus_eval_discriminate_lr_model = evals[2]\n",
    "Hungarian_eval_discriminate_lr_model = evals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Language</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>English</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>0.629291</td>\n",
       "      <td>0.599129</td>\n",
       "      <td>0.613839</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.917775</td>\n",
       "      <td>15.3079</td>\n",
       "      <td>18.618</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSta-D</td>\n",
       "      <td>German</td>\n",
       "      <td>0.363792</td>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.488972</td>\n",
       "      <td>0.525896</td>\n",
       "      <td>0.563886</td>\n",
       "      <td>0.922011</td>\n",
       "      <td>293.5972</td>\n",
       "      <td>17.371</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaNplus</td>\n",
       "      <td>Danish</td>\n",
       "      <td>0.323306</td>\n",
       "      <td>0.334995</td>\n",
       "      <td>0.348548</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>0.373774</td>\n",
       "      <td>0.923081</td>\n",
       "      <td>47.3114</td>\n",
       "      <td>11.963</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>Hungarian</td>\n",
       "      <td>0.656631</td>\n",
       "      <td>0.080340</td>\n",
       "      <td>0.141467</td>\n",
       "      <td>0.102481</td>\n",
       "      <td>0.114420</td>\n",
       "      <td>0.854914</td>\n",
       "      <td>153.7344</td>\n",
       "      <td>6.023</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset   Language  eval_loss  eval_precision  eval_recall   eval_f1  \\\n",
       "0   Baseline    English   0.288196        0.629291     0.599129  0.613839   \n",
       "1    NoSta-D     German   0.363792        0.568852     0.488972  0.525896   \n",
       "2    DaNplus     Danish   0.323306        0.334995     0.348548  0.341637   \n",
       "3  Hungarian  Hungarian   0.656631        0.080340     0.141467  0.102481   \n",
       "\n",
       "   eval_span_f1  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0      0.650794       0.917775       15.3079                   18.618   \n",
       "1      0.563886       0.922011      293.5972                   17.371   \n",
       "2      0.373774       0.923081       47.3114                   11.963   \n",
       "3      0.114420       0.854914      153.7344                    6.023   \n",
       "\n",
       "   eval_steps_per_second  \n",
       "0                  0.588  \n",
       "1                  0.545  \n",
       "2                  0.380  \n",
       "3                  0.189  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Dataset\", \"Language\"] + [name for name, _ in baseline_eval_discriminate_lr_model.items()]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Add the evals to df\n",
    "df.loc[0] = [\"Baseline\", \"English\"] + [value for _, value in baseline_eval_discriminate_lr_model.items()]\n",
    "df.loc[1] = [\"NoSta-D\", \"German\"] + [value for _, value in NoStaD_eval_discriminate_lr_model.items()]\n",
    "df.loc[2] = [\"DaNplus\", \"Danish\"] + [value for _, value in DaNplus_eval_discriminate_lr_model.items()]\n",
    "df.loc[3] = [\"Hungarian\", \"Hungarian\"] + [value for _, value in Hungarian_eval_discriminate_lr_model.items()]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval discriminate_lr and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.48s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2872945964336395, 'eval_precision': 0.6490825688073395, 'eval_recall': 0.616557734204793, 'eval_f1': 0.6324022346368714, 'eval_span_f1': 0.67595818815331, 'eval_accuracy': 0.9226835753732869, 'eval_runtime': 15.2792, 'eval_samples_per_second': 18.653, 'eval_steps_per_second': 0.589}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [04:34<00:00,  1.72s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36160358786582947, 'eval_precision': 0.5553682435991992, 'eval_recall': 0.5076567466050275, 'eval_f1': 0.5304417832343766, 'eval_span_f1': 0.5741448796114498, 'eval_accuracy': 0.921594093384969, 'eval_runtime': 276.2425, 'eval_samples_per_second': 18.462, 'eval_steps_per_second': 0.579}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:42<00:00,  2.36s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2920217216014862, 'eval_precision': 0.3894849785407725, 'eval_recall': 0.37655601659751037, 'eval_f1': 0.3829113924050632, 'eval_span_f1': 0.42701525054466233, 'eval_accuracy': 0.9272769652118175, 'eval_runtime': 44.7335, 'eval_samples_per_second': 12.653, 'eval_steps_per_second': 0.402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:30<00:00,  5.18s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5230175256729126, 'eval_precision': 0.1509223029625489, 'eval_recall': 0.20209580838323354, 'eval_f1': 0.1728, 'eval_span_f1': 0.20027201632097924, 'eval_accuracy': 0.872255489021956, 'eval_runtime': 153.7576, 'eval_samples_per_second': 6.022, 'eval_steps_per_second': 0.189}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the task and name of the pretrained model and the batch size for finetuning\n",
    "task = \"ner\"\n",
    "model_name = \"xlm-mlm-17-1280\"\n",
    "save_name = \"scheduler_AND_discriminate_lr\"\n",
    "batch_size = 32\n",
    "\n",
    "# Flag to indicate whether to label all tokens or just the first token of each word\n",
    "label_all_tokens = True\n",
    "\n",
    "# File paths to splits of the chosen dataset\n",
    "file_paths = {\n",
    "    \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "    \"validation\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "}\n",
    "\n",
    "trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "# load trianed model to trainer\n",
    "trainer.set_trainer(use_old = True)\n",
    "evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "baseline_eval_discriminate_lr_and_scheduler_model = evals[0]\n",
    "NoStaD_eval_discriminate_lr_and_scheduler_model = evals[1]\n",
    "DaNplus_eval_discriminate_lr_and_scheduler_model = evals[2]\n",
    "Hungarian_eval_discriminate_lr_and_scheduler_model = evals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Language</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>English</td>\n",
       "      <td>0.287295</td>\n",
       "      <td>0.649083</td>\n",
       "      <td>0.616558</td>\n",
       "      <td>0.632402</td>\n",
       "      <td>0.675958</td>\n",
       "      <td>0.922684</td>\n",
       "      <td>15.2792</td>\n",
       "      <td>18.653</td>\n",
       "      <td>0.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSta-D</td>\n",
       "      <td>German</td>\n",
       "      <td>0.361604</td>\n",
       "      <td>0.555368</td>\n",
       "      <td>0.507657</td>\n",
       "      <td>0.530442</td>\n",
       "      <td>0.574145</td>\n",
       "      <td>0.921594</td>\n",
       "      <td>276.2425</td>\n",
       "      <td>18.462</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaNplus</td>\n",
       "      <td>Danish</td>\n",
       "      <td>0.292022</td>\n",
       "      <td>0.389485</td>\n",
       "      <td>0.376556</td>\n",
       "      <td>0.382911</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>0.927277</td>\n",
       "      <td>44.7335</td>\n",
       "      <td>12.653</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>Hungarian</td>\n",
       "      <td>0.523018</td>\n",
       "      <td>0.150922</td>\n",
       "      <td>0.202096</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.200272</td>\n",
       "      <td>0.872255</td>\n",
       "      <td>153.7576</td>\n",
       "      <td>6.022</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset   Language  eval_loss  eval_precision  eval_recall   eval_f1  \\\n",
       "0   Baseline    English   0.287295        0.649083     0.616558  0.632402   \n",
       "1    NoSta-D     German   0.361604        0.555368     0.507657  0.530442   \n",
       "2    DaNplus     Danish   0.292022        0.389485     0.376556  0.382911   \n",
       "3  Hungarian  Hungarian   0.523018        0.150922     0.202096  0.172800   \n",
       "\n",
       "   eval_span_f1  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0      0.675958       0.922684       15.2792                   18.653   \n",
       "1      0.574145       0.921594      276.2425                   18.462   \n",
       "2      0.427015       0.927277       44.7335                   12.653   \n",
       "3      0.200272       0.872255      153.7576                    6.022   \n",
       "\n",
       "   eval_steps_per_second  \n",
       "0                  0.589  \n",
       "1                  0.579  \n",
       "2                  0.402  \n",
       "3                  0.189  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Dataset\", \"Language\"] + [name for name, _ in baseline_eval_discriminate_lr_and_scheduler_model.items()]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Add the evals to df\n",
    "df.loc[0] = [\"Baseline\", \"English\"] + [value for _, value in baseline_eval_discriminate_lr_and_scheduler_model.items()]\n",
    "df.loc[1] = [\"NoSta-D\", \"German\"] + [value for _, value in NoStaD_eval_discriminate_lr_and_scheduler_model.items()]\n",
    "df.loc[2] = [\"DaNplus\", \"Danish\"] + [value for _, value in DaNplus_eval_discriminate_lr_and_scheduler_model.items()]\n",
    "df.loc[3] = [\"Hungarian\", \"Hungarian\"] + [value for _, value in Hungarian_eval_discriminate_lr_and_scheduler_model.items()]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline_tweaked_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.46s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33056125044822693, 'eval_precision': 0.538647342995169, 'eval_recall': 0.485838779956427, 'eval_f1': 0.5108820160366552, 'eval_span_f1': 0.5526932084309134, 'eval_accuracy': 0.9003886275311924, 'eval_runtime': 15.1824, 'eval_samples_per_second': 18.772, 'eval_steps_per_second': 0.593}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [04:38<00:00,  1.74s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36150023341178894, 'eval_precision': 0.49940234281616064, 'eval_recall': 0.40238851969565637, 'eval_f1': 0.4456771027788149, 'eval_span_f1': 0.5014494339003446, 'eval_accuracy': 0.910031170303202, 'eval_runtime': 280.2202, 'eval_samples_per_second': 18.2, 'eval_steps_per_second': 0.571}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:42<00:00,  2.39s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3544788956642151, 'eval_precision': 0.3442622950819672, 'eval_recall': 0.28319502074688796, 'eval_f1': 0.31075697211155373, 'eval_span_f1': 0.35558180744240997, 'eval_accuracy': 0.9213915273002738, 'eval_runtime': 45.3007, 'eval_samples_per_second': 12.494, 'eval_steps_per_second': 0.397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:23<00:00,  4.95s/it]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6593185067176819, 'eval_precision': 0.09040444091990484, 'eval_recall': 0.1279940119760479, 'eval_f1': 0.10596436870642914, 'eval_span_f1': 0.13971323774591532, 'eval_accuracy': 0.8505557691955539, 'eval_runtime': 147.1266, 'eval_samples_per_second': 6.294, 'eval_steps_per_second': 0.197}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the task and name of the pretrained model and the batch size for finetuning\n",
    "task = \"ner\"\n",
    "model_name = \"xlm-mlm-17-1280\"\n",
    "save_name = \"baseline_tweaked_lr\"\n",
    "batch_size = 32\n",
    "\n",
    "# Flag to indicate whether to label all tokens or just the first token of each word\n",
    "label_all_tokens = True\n",
    "\n",
    "# File paths to splits of the chosen dataset\n",
    "file_paths = {\n",
    "    \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "    \"validation\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "}\n",
    "\n",
    "trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "# load trianed model to trainer\n",
    "trainer.set_trainer(use_old = True)\n",
    "evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "baseline_eval_baseline_tweaked_lr_model = evals[0]\n",
    "NoStaD_eval_baseline_tweaked_lr_model = evals[1]\n",
    "DaNplus_eval_baseline_tweaked_lr_model = evals[2]\n",
    "Hungarian_eval_baseline_tweaked_lr_model = evals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Language</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>English</td>\n",
       "      <td>0.330561</td>\n",
       "      <td>0.538647</td>\n",
       "      <td>0.485839</td>\n",
       "      <td>0.510882</td>\n",
       "      <td>0.552693</td>\n",
       "      <td>0.900389</td>\n",
       "      <td>15.1824</td>\n",
       "      <td>18.772</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoSta-D</td>\n",
       "      <td>German</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.499402</td>\n",
       "      <td>0.402389</td>\n",
       "      <td>0.445677</td>\n",
       "      <td>0.501449</td>\n",
       "      <td>0.910031</td>\n",
       "      <td>280.2202</td>\n",
       "      <td>18.200</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaNplus</td>\n",
       "      <td>Danish</td>\n",
       "      <td>0.354479</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.283195</td>\n",
       "      <td>0.310757</td>\n",
       "      <td>0.355582</td>\n",
       "      <td>0.921392</td>\n",
       "      <td>45.3007</td>\n",
       "      <td>12.494</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>Hungarian</td>\n",
       "      <td>0.659319</td>\n",
       "      <td>0.090404</td>\n",
       "      <td>0.127994</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.139713</td>\n",
       "      <td>0.850556</td>\n",
       "      <td>147.1266</td>\n",
       "      <td>6.294</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset   Language  eval_loss  eval_precision  eval_recall   eval_f1  \\\n",
       "0   Baseline    English   0.330561        0.538647     0.485839  0.510882   \n",
       "1    NoSta-D     German   0.361500        0.499402     0.402389  0.445677   \n",
       "2    DaNplus     Danish   0.354479        0.344262     0.283195  0.310757   \n",
       "3  Hungarian  Hungarian   0.659319        0.090404     0.127994  0.105964   \n",
       "\n",
       "   eval_span_f1  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0      0.552693       0.900389       15.1824                   18.772   \n",
       "1      0.501449       0.910031      280.2202                   18.200   \n",
       "2      0.355582       0.921392       45.3007                   12.494   \n",
       "3      0.139713       0.850556      147.1266                    6.294   \n",
       "\n",
       "   eval_steps_per_second  \n",
       "0                  0.593  \n",
       "1                  0.571  \n",
       "2                  0.397  \n",
       "3                  0.197  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Dataset\", \"Language\"] + [name for name, _ in baseline_eval_baseline_tweaked_lr_model.items()]\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Add the evals to df\n",
    "df.loc[0] = [\"Baseline\", \"English\"] + [value for _, value in baseline_eval_baseline_tweaked_lr_model.items()]\n",
    "df.loc[1] = [\"NoSta-D\", \"German\"] + [value for _, value in NoStaD_eval_baseline_tweaked_lr_model.items()]\n",
    "df.loc[2] = [\"DaNplus\", \"Danish\"] + [value for _, value in DaNplus_eval_baseline_tweaked_lr_model.items()]\n",
    "df.loc[3] = [\"Hungarian\", \"Hungarian\"] + [value for _, value in Hungarian_eval_baseline_tweaked_lr_model.items()]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
