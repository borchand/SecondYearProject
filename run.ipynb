{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TokenClassificationTrainer import TokenClassificationTrainer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs(discriminate_lr = False, scheduler = False, save_name = \"baseline\"):\n",
    "    # Set the task and name of the pretrained model and the batch size for finetuning\n",
    "    task = \"ner\"\n",
    "    model_name = \"xlm-mlm-17-1280\"  # \"bert-base-multilingual-cased\" or \"xlm-mlm-17-1280\"\n",
    "    seed = np.random.randint(0, 1000)\n",
    "    save_name = save_name + \".seed-\" + str(seed)\n",
    "    batch_size = 32\n",
    "\n",
    "    # Flag to indicate whether to label all tokens or just the first token of each word\n",
    "    label_all_tokens = True\n",
    "\n",
    "    # File paths to splits of the chosen dataset\n",
    "    file_paths = {\n",
    "        \"train\": \"data/datasets/baseline/en_ewt_nn_train.conll\",\n",
    "        \"validation\": \"data/datasets/baseline/en_ewt_nn_newsgroup_dev.conll\",\n",
    "    }\n",
    "\n",
    "    # initialize trainer\n",
    "    trainer = TokenClassificationTrainer(task, model_name, save_name, batch_size, label_all_tokens, file_paths)\n",
    "\n",
    "\n",
    "    # Training\n",
    "    trainer.train_and_save(discriminate_lr = False, scheduler = False, seed = seed)\n",
    "\n",
    "    evals = trainer.evaluate_multiple([\"data/datasets/baseline/en_ewt_nn_newsgroup_test.conll\", \"data/datasets/NoSta-D/NER-de-test.tsv\", \"data/datasets/DaNplus/da_news_test.tsv\", \"data/datasets/hungarian/hungarian_test.tsv\"])\n",
    "\n",
    "    baseline_eval_baseline_model = evals[0]\n",
    "    NoStaD_eval_baseline_model = evals[1]\n",
    "    DaNplus_eval_baseline_model = evals[2]\n",
    "    Hungarian_eval_baseline_model = evals[3]\n",
    "\n",
    "    cols = [\"Dataset\", \"Language\", \"Seed\"] + [name for name, _ in baseline_eval_baseline_model.items()]\n",
    "\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Add the evals to df\n",
    "    df.loc[0] = [\"Baseline\", \"English\", seed] + [value for _, value in baseline_eval_baseline_model.items()]\n",
    "    df.loc[1] = [\"NoSta-D\", \"German\", seed] + [value for _, value in NoStaD_eval_baseline_model.items()]\n",
    "    df.loc[2] = [\"DaNplus\", \"Danish\", seed] + [value for _, value in DaNplus_eval_baseline_model.items()]\n",
    "    df.loc[3] = [\"Hungarian\", \"Hungarian\", seed] + [value for _, value in Hungarian_eval_baseline_model.items()]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_baseline = []\n",
    "list_discriminate_lr = []\n",
    "list_scheduler = []\n",
    "list_both = []\n",
    "for i in range(10):\n",
    "    list_baseline.append(runs(save_name = \"baseline\"))\n",
    "    list_discriminate_lr.append(runs(discriminate_lr = True, save_name = \"discriminate_lr\"))\n",
    "    list_scheduler.append(runs(scheduler = True, save_name = \"scheduler\"))\n",
    "    list_both.append(runs(discriminate_lr = True, scheduler = True, save_name = \"both\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
